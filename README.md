# Artificial_Intelligence
 # In main.py simple read of pdf is done (Pdfplumber library is used)
# In chunk_1.py phrase level chunking is done.
# In chunking.py Sentence level chunking is done.
# In lemmentatization.py first the POS tags are assigned to each word NN->noun , VB->verb, etc and then performing lemmentization processing on each word respectively.
# In Stp.py fitz library is used for better extraction of words. NLTk library is used for tokenizing. stopwords are find and compared with extracted text. If there is any stopword like 'The', 'of' etc is removed from extracted text.
# In tokenization.py tokenization processing is done for each sentence separately.